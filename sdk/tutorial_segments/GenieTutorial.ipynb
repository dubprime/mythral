{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://files.intelligent-artifacts.com/horizontal-black-green@2x.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Hello, World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update your agent information, and then connect to it\n",
    "from ia.gaius.agent_client import AgentClient\n",
    "\n",
    "api_key = '<YOUR-API-KEY-HERE>'\n",
    "name    = '<YOUR-AGENT-NAME-HERE>'\n",
    "domain  = 'localhost'\n",
    "secure  = True\n",
    "\n",
    "agent_info = {'api_key': api_key,\n",
    "               'name':  name,\n",
    "               'domain': domain,\n",
    "               'secure': secure}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_info = {'api_key': 'ABCD-1234',\n",
    "              'name': '',\n",
    "              'domain': 'gaius-api:80',\n",
    "              'secure': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'connection': 'okay', 'agent': 'simple'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = AgentClient(agent_info)\n",
    "agent.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'p46b6b076c', 'name': 'P1'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use only one node\n",
    "ingress_nodes = ['P1']\n",
    "query_nodes   = ['P1']\n",
    "\n",
    "agent.set_ingress_nodes(ingress_nodes)\n",
    "agent.set_query_nodes(query_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wm-cleared'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear working memory\n",
    "agent.clear_wm(nodes=ingress_nodes)\n",
    "# agent.clear_wm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_1 = [{\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {}}, \n",
    "              {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}}]\n",
    "\n",
    "# Observe a sequence of events\n",
    "for event in sequence_1:\n",
    "    agent.observe(data=event,nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MODEL|7d0678ba6305341ce0d25133ab086208656a562f'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learn a sequence\n",
    "agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'observed'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe new data\n",
    "agent.observe(data={\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {}},nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'confidence': 1,\n",
       "  'confluence': 0.5,\n",
       "  'emotives': {},\n",
       "  'entropy': 0.5,\n",
       "  'evidence': 0.5,\n",
       "  'extras': [],\n",
       "  'fragmentation': 0,\n",
       "  'frequency': 3,\n",
       "  'future': [['world']],\n",
       "  'grand_hamiltonian': 0.5,\n",
       "  'hamiltonian': 0,\n",
       "  'itfdf_similarity': 1,\n",
       "  'matches': ['hello'],\n",
       "  'missing': [],\n",
       "  'name': '7d0678ba6305341ce0d25133ab086208656a562f',\n",
       "  'past': [],\n",
       "  'potential': 3.5,\n",
       "  'present': [['hello']],\n",
       "  'similarity': 0.666666687,\n",
       "  'snr': 1,\n",
       "  'type': 'prototypical'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Predictions\n",
    "agent.get_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaius Tutorial \n",
    "\n",
    "This is a tutorial for Intelligent Artifacts. It is a general evolving networked intelligence engine, i.e. an Artificial General Intelligence platform, built on top of Intelligent Artifacts' GAIuS AGI framework. In order to proceed with this tutorial, you need to sign up for access to the Gaius platform. Please contact your Intelligent Artifacts representative if you need help getting access to the Gaius platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaius Tutorial Overview"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Create a Genome Topology for your Agent\n",
    "2. Spawn a Bottle for your Agent\n",
    "3. Connect to your Agent\n",
    "4. Interact with your Agent using String Data\n",
    "    5a. Data Preparation\n",
    "    5b. Observing Data\n",
    "    5c. Learning Data\n",
    "    5d. Get Predictions\n",
    "5. An Emotional Machine: Emotives and Mood\n",
    "6. Numerical Data: Vectors\n",
    "7. Attaching Manipulatives\n",
    "    7a. Manipulatives\n",
    "    7b. Abstractions and Deep Learning\n",
    "8. Gaius Examples\n",
    "    8a. Classification\n",
    "    8b. Utility Polarity\n",
    "    8c. Utility Value\n",
    "9. Advanced Gaius Examples\n",
    "    9a. Building a Recommendation Engine with Emotives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create a Genome Topology for your Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a agent you first need to create its genome. An agent's genome is very much like a genome in biology where it is an organism's complete set of genetic instructions. In this case, the genome is for an AI agent. A genome can be created manually, algorithmically, or using the Intelligent Artifacts Lab GUI (Lab). For now let's a genome that is located in the config folder named, 'demo-all.' All you need to do is upload the Genome in the Lab's Topology Creator. Moreover, a genome can be downloaded and uploaded for later use or sharing after spawning an agent (covered in the next section below). The genome file is a JavaScript Object Notation (JSON) string that can be imported as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ia.gaius.genome_info import Genome\n",
    "import json\n",
    "\n",
    "genome_topology = '''{\"elements\":{\"nodes\":[{\"data\":{\"name\":\"P1\",\"id\":\"p7ae0a0612\",\"type\":\"primitive\",\"sources\":[\"observables\"],\"manipulatives\":[],\"always_update_frequencies\":false,\"auto_act_method\":\"none\",\"auto_act_threshold\":0.8,\"auto_learn_method\":\"none\",\"classifier\":\"CVC\",\"datastore\":\"mongodb\",\"dynamic_sequence_length\":true,\"max_sequence_length\":0,\"sort\":true,\"process_predictions\":true,\"quiescence\":3,\"search_depth\":10,\"recall_threshold\":0.1,\"smoothness\":3,\"max_predictions\":100,\"persistence\":5},\"position\":{\"x\":587,\"y\":272.5975},\"group\":\"nodes\",\"removed\":false,\"selected\":false,\"selectable\":false,\"locked\":false,\"grabbable\":true,\"pannable\":false,\"classes\":\"\"},{\"data\":{\"name\":\"P2\",\"id\":\"p5b55a8399\",\"type\":\"primitive\",\"sources\":[\"m44ee0fd21\"],\"manipulatives\":[\"m44ee0fd21\"],\"always_update_frequencies\":false,\"auto_act_method\":\"none\",\"auto_act_threshold\":0.8,\"auto_learn_method\":\"none\",\"classifier\":\"CVC\",\"datastore\":\"mongodb\",\"dynamic_sequence_length\":true,\"max_sequence_length\":0,\"sort\":true,\"process_predictions\":true,\"quiescence\":3,\"search_depth\":10,\"recall_threshold\":0.1,\"smoothness\":3,\"max_predictions\":100,\"persistence\":5},\"position\":{\"x\":687,\"y\":272.5975},\"group\":\"nodes\",\"removed\":false,\"selected\":false,\"selectable\":false,\"locked\":false,\"grabbable\":true,\"pannable\":false,\"classes\":\"\"},{\"data\":{\"name\":\"P3\",\"id\":\"p85643c122\",\"type\":\"primitive\",\"sources\":[\"observables\"],\"manipulatives\":[\"mb94faab95\"],\"always_update_frequencies\":false,\"auto_act_method\":\"none\",\"auto_act_threshold\":0.8,\"auto_learn_method\":\"none\",\"classifier\":\"CVC\",\"datastore\":\"mongodb\",\"dynamic_sequence_length\":true,\"max_sequence_length\":0,\"sort\":true,\"process_predictions\":true,\"quiescence\":3,\"search_depth\":10,\"recall_threshold\":0.1,\"smoothness\":3,\"max_predictions\":100,\"persistence\":5},\"position\":{\"x\":789,\"y\":271.5975},\"group\":\"nodes\",\"removed\":false,\"selected\":false,\"selectable\":false,\"locked\":false,\"grabbable\":true,\"pannable\":false,\"classes\":\"\"},{\"data\":{\"displayName\":\"Multiplier\",\"mtype\":\"normal\",\"name\":\"emotiveMultiplier\",\"inputs\":[\"strings\",\"emotives\",\"vectors\"],\"outputs\":[\"strings\",\"emotives\",\"vectors\",\"predictions\"],\"description\":\"Multiplies emotive values by a configurable value.\",\"tags\":[\"emotives\",\"multiply\",\"multiplication\",\"multi\",\"value\",\"factor\"],\"category\":\"emotives\",\"genes\":{\"sources\":{\"alleles\":[],\"mutability\":0,\"value\":[\"observables\"],\"volatility\":0},\"multiplier\":{\"alleles\":[1,2,3,4,5,6,7,8,9,10],\"mutability\":0,\"value\":5,\"volatility\":0}},\"id\":\"m44ee0fd21\",\"type\":\"manipulative\",\"primitive\":\"p5b55a8399\"},\"position\":{\"x\":687,\"y\":372.5975},\"group\":\"nodes\",\"removed\":false,\"selected\":false,\"selectable\":false,\"locked\":false,\"grabbable\":true,\"pannable\":false,\"classes\":\"\"},{\"data\":{\"displayName\":\"abstraction\",\"name\":\"abstraction\",\"category\":\"standard\",\"mtype\":\"output\",\"description\":\"Output manipulative to forward abstracted predictions to other Primitives. The chosen field of the predictions are forwarded as. The are also forwarded. No are forwarded.\",\"tags\":[\"abstraction\",\"name\",\"present\",\"past\",\"future\",\"missing\",\"matches\",\"extras\",\"output\",\"forward\",\"abstract\",\"emotives\",\"strings\",\"emotive\",\"classification\"],\"genes\":{\"sources\":{\"alleles\":[],\"mutability\":0,\"value\":[\"cognition-data\"],\"volatility\":0},\"primitives\":{\"alleles\":[],\"mutability\":0,\"value\":[\"pc4dff12d6\"],\"volatility\":0},\"field\":{\"alleles\":[\"name\",\"past\",\"present\",\"future\",\"missing\",\"extras\",\"matches\",\"classification\"],\"mutability\":0,\"value\":[\"name\"],\"volatility\":0}},\"id\":\"mb94faab95\",\"type\":\"manipulative\",\"primitive\":\"p85643c122\"},\"position\":{\"x\":789,\"y\":171.59750000000003},\"group\":\"nodes\",\"removed\":false,\"selected\":false,\"selectable\":false,\"locked\":false,\"grabbable\":true,\"pannable\":false,\"classes\":\"\"},{\"data\":{\"name\":\"P4\",\"id\":\"pc4dff12d6\",\"type\":\"primitive\",\"sources\":[\"observables\"],\"manipulatives\":[],\"always_update_frequencies\":false,\"auto_act_method\":\"none\",\"auto_act_threshold\":0.8,\"auto_learn_method\":\"none\",\"classifier\":\"CVC\",\"datastore\":\"mongodb\",\"dynamic_sequence_length\":true,\"max_sequence_length\":0,\"sort\":true,\"process_predictions\":true,\"quiescence\":3,\"search_depth\":10,\"recall_threshold\":0.1,\"smoothness\":3,\"max_predictions\":100,\"persistence\":5},\"position\":{\"x\":788,\"y\":81.59750000000003},\"group\":\"nodes\",\"removed\":false,\"selected\":false,\"selectable\":false,\"locked\":false,\"grabbable\":true,\"pannable\":false,\"classes\":\"\"}],\"edges\":[{\"data\":{\"id\":\"eb7965422f\",\"source\":\"m44ee0fd21\",\"target\":\"p5b55a8399\",\"type\":\"edge\"},\"position\":{\"x\":0,\"y\":0},\"group\":\"edges\",\"removed\":false,\"selected\":false,\"selectable\":false,\"locked\":false,\"grabbable\":true,\"pannable\":true,\"classes\":\"\"},{\"data\":{\"id\":\"ec26380337\",\"source\":\"p85643c122\",\"target\":\"mb94faab95\",\"type\":\"edge\"},\"position\":{\"x\":0,\"y\":0},\"group\":\"edges\",\"removed\":false,\"selected\":false,\"selectable\":false,\"locked\":false,\"grabbable\":true,\"pannable\":true,\"classes\":\"abstractionEdge\"},{\"data\":{\"id\":\"e5dd3a5044\",\"source\":\"mb94faab95\",\"target\":\"pc4dff12d6\",\"type\":\"edge\"},\"position\":{\"x\":0,\"y\":0},\"group\":\"edges\",\"removed\":false,\"selected\":false,\"selectable\":false,\"locked\":false,\"grabbable\":true,\"pannable\":true,\"classes\":\"abstractionEdge\"}]},\"style\":[{\"selector\":\"edge\",\"style\":{\"target-arrow-shape\":\"triangle\",\"curve-style\":\"straight\",\"control-point-distances\":10,\"control-point-weights\":0.1,\"width\":1,\"overlay-color\":\"#fff\"}},{\"selector\":\"edge.curved\",\"style\":{\"curve-style\":\"bezier\"}}],\"data\":{},\"zoomingEnabled\":true,\"userZoomingEnabled\":true,\"zoom\":1,\"minZoom\":1e-50,\"maxZoom\":1,\"panningEnabled\":true,\"userPanningEnabled\":true,\"pan\":{\"x\":0,\"y\":0},\"boxSelectionEnabled\":false,\"renderer\":{\"name\":\"canvas\"},\"wheelSensitivity\":0.1,\"motionBlur\":false,\"agent\":\"demo-all\",\"description\":\"tutorial\"}'''\n",
    "genome = Genome(json.loads(genome_topology))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter values in genomes are intended to be modified. Since the genome file is a JSON object, modifying values is easy. You can also use the Genome object class to easily find nodes and values for modification.\n",
    "\n",
    "The following example will change the `'recall_threshold'` and `'max_predictions'` of the primitive 'P1':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4:       \n",
      " ID               : pc4dff12d6       \n",
      " Recall Threshold : 0.1       \n",
      " Max Predictions  : 100\n"
     ]
    }
   ],
   "source": [
    "for p in genome.primitives.values():\n",
    "    if p['name'] == 'P1':\n",
    "        p['recall_threshold'] = 0.1\n",
    "        p['max_predictions'] = 10\n",
    "        \n",
    "print(\"%s: \\\n",
    "      \\n ID               : %s \\\n",
    "      \\n Recall Threshold : %s \\\n",
    "      \\n Max Predictions  : %s\" %(p['name'], p['id'], p['recall_threshold'], p['max_predictions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deploy an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a Genome has been uploaded or created on lab, it is possible to deploy an agent. In the sidebar, select the Genome from the Perception Manifold area. Find the Genome named 'demo-all', next select an agent size, then select 'M' for 'medium'. This will deploy an agent using the Genome topology. On the dashboard page you can see the status of your agent as it is created.\n",
    "\n",
    "An agent is a computer in the cloud (server, virtual machine) or local machine preconfigured with the GAIuS operating system. On IA's cloud, this is done automatically through the Lab when spawning a new agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few minutes, a new agent should appear in the 'Deployed Agents' area of the sidebar. Click on your agent. If following along, copy and paste the <b>agent name</b> and the <b>api key</b> into the agent_info below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Connect to Your Agent\n",
    "\n",
    "You have options on how to connect to a running agent. You can use the provided SDK in your preferred language, or roll your own.\n",
    "\n",
    "### Use the SDK's AgentClient\n",
    "\n",
    "Install the Python version of the SDK via:\n",
    "\n",
    "`pip3 install ia-sdk`\n",
    "\n",
    "or, if using the Jupyter for Intelligent Artifacts container, open a terminal and update it with:\n",
    "\n",
    "`pip3 install --user -U ia-sdk`\n",
    "\n",
    "To check the current sdk version:\n",
    "\n",
    "`pip3 show ia-sdk`\n",
    "\n",
    "The AgentClient in the SDK requires only the URL and api key.  Once you connect, you can define the ingress and query nodes for the agent.  Each AgentClient can define its own ingress and query nodes.  This allows multiple inputs sources to connect to different ingress nodes of the agent, as well as allowing to query different Cognitive Processors for answers.\n",
    "\n",
    "For example, to connect to a cloud agent:\n",
    "\n",
    "~~~\n",
    "\n",
    "# update your bottle information, and then connect to it\n",
    "api_key = '<secret-api-key-here>'\n",
    "name    = '<bottle-name-here>'\n",
    "domain  = 'intelligent-artifacts.com'\n",
    "secure  = True\n",
    "\n",
    "from ia.gaius.agent_client import AgentClient\n",
    "\n",
    "agent_info = {'api_key': api_key,\n",
    "               'name':  name,\n",
    "               'domain': domain,\n",
    "               'secure': secure}\n",
    "agent = AgentClient(agent_info)\n",
    "agent.connect()\n",
    "agent\n",
    "\n",
    "~~~\n",
    "\n",
    "To connect to a local agent:\n",
    "\n",
    "~~~\n",
    "\n",
    "from ia.gaius.agent_client import AgentClient\n",
    "\n",
    "agent_info = {'api_key': 'ABCD-1234',\n",
    "               'name':  '',\n",
    "               'domain': '<agent>:<port>',\n",
    "               'secure': False}\n",
    "agent = AgentClient(agent_info)\n",
    "agent.connect()\n",
    "agent\n",
    "\n",
    "~~~\n",
    "For local agents it may be necessary to connect to the docker network, if connecting via another docker container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, roll your own connections\n",
    "To roll your own, you simply need the agent's host and domain names, the secret API key, and the IDs of each primitive you'd like to connect.  You can use a library like Python's `requests` to POST data and query the primitives.\n",
    "\n",
    "For example:\n",
    "\n",
    "Let's connect to either a local or a cloud agent\n",
    "\n",
    "~~~\n",
    "import requests\n",
    "\n",
    "# Local Agent Info\n",
    "name    = 'agent'\n",
    "api_key = 'ABCD-1234'\n",
    "\n",
    "# connect to local agent with http \n",
    "requests.get(f\"http://{name}/connect\", \n",
    "             headers={'X-API-KEY': api_key}).json()\n",
    "             \n",
    "# Cloud Agent Info\n",
    "name    = '<agent-name-here>'\n",
    "domain  = 'intelligent-artifacts.com'\n",
    "api_key = '<secret-api-key-here>'\n",
    "\n",
    "# connect to a cloud agent with https and domain\n",
    "requests.get(f\"https://{name}.{domain}/connect\", \n",
    "             headers={'X-API-KEY': api_key}).json()\n",
    "~~~\n",
    "\n",
    "To observe data:\n",
    "\n",
    "In order to observe data you need to have the primitive ids of the Cognitive Processors. If you don't know the ids, you can retrieve them by connecting to the agent with request first.\n",
    "\n",
    "The following examples are for cloud bottles.\n",
    "\n",
    "~~~\n",
    "# data as a GDF you want to observe\n",
    "data = {\"strings\": [\"hello\",\"world\"], \"vectors\": [], \"emotives\": {}}\n",
    "\n",
    "agent_connection_info = requests.get(f\"https://{name}.{domain}/connect\", \n",
    "             headers={'X-API-KEY': api_key}).json()\n",
    "\n",
    "# navigate into the bottle_connection_info dictionary to get primitive ids from genome data\n",
    "primitive_id = agent_connection_info['genome']['elements']['nodes'][0]['data']['id']\n",
    "\n",
    "# observe data\n",
    "requests.post(f\"https://{name}.{domain}/{primitive_id}/observe\", \n",
    "              headers={'X-API-KEY': api_key}, \n",
    "              json={'data': data}).json()\n",
    "~~~\n",
    "\n",
    "To learn data:\n",
    "\n",
    "After you've observed a few events\n",
    "\n",
    "~~~\n",
    "\n",
    "requests.post(f\"https://{name}.{domain}/{primitive_id}/learn\", \n",
    "              headers={'X-API-KEY': api_key}).json()\n",
    "\n",
    "~~~\n",
    "\n",
    "To query for answers:\n",
    "\n",
    "After you've observed additional data, one can try to get predictions\n",
    "\n",
    "~~~\n",
    "\n",
    "# data as a GDF you want to observe in order to get predictions\n",
    "data = {\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {}}\n",
    "\n",
    "# observe data\n",
    "requests.post(f\"https://{name}.{domain}/{primitive_id}/observe\", \n",
    "              headers={'X-API-KEY': api_key}, \n",
    "              json={'data': data}).json()\n",
    "\n",
    "requests.post(f\"https://{name}.{domain}/{primitive_id}/predictions\", \n",
    "              headers={'X-API-KEY': api_key}).json()\n",
    "~~~\n",
    "\n",
    "To get a fresh start, one can\n",
    "\n",
    "clear all memory of the Cognitive Processor:\n",
    "\n",
    "\n",
    "\n",
    "~~~\n",
    "              \n",
    "requests.post(f\"https://{name}.{domain}/{primitive_id}/clear-all-memory\", \n",
    "              headers={'X-API-KEY': api_key}).json()              \n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sdk version used in this tutorial\n",
    "!pip3 show ia-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update your agent information, and then connect to it\n",
    "from ia.gaius.agent_client import AgentClient\n",
    "\n",
    "api_key = '<YOUR-API-KEY-HERE>'\n",
    "name    = '<YOUR-AGENT-NAME-HERE>'\n",
    "domain  = 'intelligent-artifacts.com'\n",
    "secure  = True\n",
    "\n",
    "agent_info = {'api_key': api_key,\n",
    "               'name':  name,\n",
    "               'domain': domain,\n",
    "               'secure': secure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_info = {'api_key': 'ABCD-1234',\n",
    "              'name': '',\n",
    "              'domain': 'gaius-api:80',\n",
    "              'secure': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<.gaius-api:80| secure: False, connected: True, gaius_agent: simple,                   ingress_nodes: 0, query_nodes: 0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = AgentClient(agent_info)\n",
    "agent.connect()\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'p46b6b076c', 'name': 'P1'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use only one node\n",
    "ingress_nodes = ['P1']\n",
    "query_nodes   = ['P1']\n",
    "\n",
    "agent.set_ingress_nodes(ingress_nodes)\n",
    "agent.set_query_nodes(query_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Interact with your Agent using String Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Data Preparation\n",
    "\n",
    "All data provided to a GAIuS Agent is supplied as <i>sequences of events</i>. The way a sequence is delimited depends on the problem domain\n",
    "\n",
    "An <b>event</b> is a single occurrence of a process.\n",
    "\n",
    "A <b>sequence</b> is a particular order in which related events, movements, or things follow each other.\n",
    "\n",
    "<i>sequence  =  [event_1, event_2, ..., event_n]</i>\n",
    "\n",
    "Even problems that don't initially appear to be sequence problems can be cast into this form.  For example, customer profile data can be converted into a sequence of a single event.  It is common to call such single-evented sequences <b>profile data</b> to distinguish them from other sequences.\n",
    "\n",
    "<i>sequence = [event_1]</i>\n",
    "\n",
    "can be represented as\n",
    "\n",
    "<i>sequence = [profile_data]</i>\n",
    "\n",
    "Each <b>event</b> must be formatted into our <b>universal input format</b>, the <b>G</b>AIuS <b>D</b>ata <b>F</b>ormat (<b>GDF</b>) object.\n",
    "\n",
    "The GDF object is a JSON object that has three core fields: <b>strings</b>, <b>vectors</b>, and <b>emotives</b>. Each field's value is an array of relevant data. All three fields must be provided even if they are empty. GDFs are easily shared between systems because they are JSON objects.\n",
    "\n",
    "Here is an empty GDF:\n",
    "\n",
    "~~~\n",
    "{\"strings\": [], \"vectors\": [], \"emotives\": {}}\n",
    "~~~\n",
    "\n",
    "In general and simply put, the strings field is for text-based data, the vectors field is for arrays of numerical data, and the emotives field is for emotional data, a non-vector number, positive or negative. The GDF object is the key for using our AGI platform. Practically any data can be converted into this form.\n",
    "\n",
    "For now let's focus on <b>string</b> data:\n",
    "\n",
    "A <b>string</b> is simply any character, e.g. \"h\", or sequential characters, \"hello\". Strings must be properly delimited with double-quotes. This conforms with the JSON standard for strings.\n",
    "\n",
    "GDFs provide the ability to observe single events.  Multiple events build sequences. For example, the following is one sequence:\n",
    "\n",
    "~~~\n",
    " [{\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {}}, \n",
    "  {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}}]\n",
    "~~~\n",
    "\n",
    "This sequence consists of two events.  The first event consists of the symbol \"hello\" in the `\"strings\"` field.  The second event consists of the symbol \"world\" in the `\"strings\"` field.\n",
    "\n",
    "Here's another example of a sequence:\n",
    "\n",
    "~~~\n",
    " [{\"strings\": [\"goodbye\"], \"vectors\": [], \"emotives\": {}},\n",
    "  {\"strings\": [\"cruel\"], \"vectors\": [], \"emotives\": {}},\n",
    "  {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}}]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the ingress and query nodes earlier, let's use your AgentClient to send data and query the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Observing data\n",
    "\n",
    "Sending data requires the `observe` API call.  The AgentClient provides this using a similarly named function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to acquire and process data you need to first make an observation. When the GAIuS Agent observes data it observes each event of a sequence one-by-one where the data is processed into the <b>working memory (WM)</b>. Data processed into the WM can later be used to make predictions or be learned where it will update its <b>knowlege base (KB)</b>. Data stored in the KB will be used to pattern match and make predictions. Analogously, you can think of WM as short term memory and KB as long term memory. It is good practice to clear the WM before the Agent observes new data to ensure that the state of the Agent is as expected.\n",
    "\n",
    "Let's follow this sequence of steps to observe and process new data:\n",
    "\n",
    "1. agent.clear_wm()       - clear wm to make sure nothing is in the wm\n",
    "2. agent.observe(event)   - observe new data\n",
    "3. agent.get_wm()         - to look at what's in the wm after an observation was made\n",
    "4. agent.clear_all_memory - when we want a fresh start we would want to clear everything in the wm and the kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wm-cleared'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.clear_wm(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'observed'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.observe(data={\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {}},nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_wm(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all-cleared'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.clear_all_memory(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for the Gaius platform is for it to observe data, learn from it, and to provide predictions.  Let's go ahead and put this all together using what we've learned above.\n",
    "\n",
    "Let's train on the two sequences:\n",
    "\n",
    "     [{\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {}}, \n",
    "      {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}}]\n",
    "\n",
    "and\n",
    "\n",
    "     [{\"strings\": [\"goodbye\"], \"vectors\": [], \"emotives\": {}},\n",
    "      {\"strings\": [\"cruel\"], \"vectors\": [], \"emotives\": {}},\n",
    "      {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe 1st Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.clear_all_memory(nodes=ingress_nodes) ### Start with a clean slate.\n",
    "\n",
    "sequence_1 = [{\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {}}, \n",
    "              {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}}]\n",
    "\n",
    "for event in sequence_1:\n",
    "    agent.observe(data=event,nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello'], ['world']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_wm(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn 1st Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MODEL|7d0678ba6305341ce0d25133ab086208656a562f'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first sequence is fully in working memory (WM) now.  Let's learn it!\n",
    "agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe 2nd Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's now observe and learn the next sequence.\n",
    "\n",
    "## Just in case, let's clear out the working memory (WM), though 'learn' does that for us already.  \n",
    "## Don't clear out ALL the memory, otherwise it will forget the sequences it learned!\n",
    "agent.clear_wm(nodes=ingress_nodes)\n",
    "\n",
    "sequence_2 = [{\"strings\": [\"goodbye\"], \"vectors\": [], \"emotives\": {}},\n",
    "              {\"strings\": [\"cruel\"], \"vectors\": [], \"emotives\": {}},\n",
    "              {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}}]\n",
    "\n",
    "for event in sequence_2:\n",
    "    agent.observe(data=event,nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn 2nd Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MODEL|3b5c9cdc4424988308922d2ec8c7bc06b7c6ac21'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The second sequence is fully in working memory (WM) now.  Let's learn it!\n",
    "agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we've taught the agent two sequences. Let us check how much sequences were learned in its kb.\n",
    "Now let's now observe new data, and request some predictions back:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Agent Status\n",
    "\n",
    "Sometimes it is helpful to view information about the agent's current state, especially how many sequences are in the KB.\n",
    "You can do this using the API command agent.show_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PREDICT': True,\n",
       " 'SLEEPING': False,\n",
       " 'emotives': {},\n",
       " 'last_learned_model_name': '3b5c9cdc4424988308922d2ec8c7bc06b7c6ac21',\n",
       " 'models_kb': '{KB| objects: 2}',\n",
       " 'name': 'P1',\n",
       " 'size_WM': 0,\n",
       " 'target': '',\n",
       " 'time': 5,\n",
       " 'vectors_kb': '{KB| objects: 0}'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{KB| objects: 2}'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at how many sequences the agent has learned\n",
    "agent.show_status(nodes=ingress_nodes)['models_kb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4d. Get Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe 1st event in New Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'observed'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Again, we want to ensure there's nothing in the working memory (WM) before we send it new sequences:\n",
    "agent.clear_wm(nodes=ingress_nodes)\n",
    "data = {\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {}}\n",
    "agent.observe(data=data,nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'confidence': 1,\n",
       "  'confluence': 0.4,\n",
       "  'emotives': {},\n",
       "  'entropy': 0.464385629,\n",
       "  'evidence': 0.5,\n",
       "  'extras': [],\n",
       "  'fragmentation': 0,\n",
       "  'frequency': 1,\n",
       "  'future': [['world']],\n",
       "  'grand_hamiltonian': 0.232192814,\n",
       "  'hamiltonian': 0,\n",
       "  'itfdf_similarity': 1,\n",
       "  'matches': ['hello'],\n",
       "  'missing': [],\n",
       "  'name': '7d0678ba6305341ce0d25133ab086208656a562f',\n",
       "  'past': [],\n",
       "  'potential': 3.5,\n",
       "  'present': [['hello']],\n",
       "  'similarity': 0.666666687,\n",
       "  'snr': 1,\n",
       "  'type': 'prototypical'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_predictions(nodes=query_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'P1' has provided an ensemble of predictions for us!  This ensemble, however, only has one prediction.  The prediction is the first sequence that we taught it.  It is represented as a <b>prediction object</b>.  This is our <b>universal output format</b>.  Let's look at some of the fields and values it provides:\n",
    "\n",
    "\n",
    "It states that it has matched on the `'hello'` symbol:\n",
    "\n",
    "    'matches': ['hello']\n",
    "\n",
    "From this, it believes that the current present state of affairs is:\n",
    "\n",
    "    'present': [['hello']]\n",
    "\n",
    "That's one event with the `'hello'` symbol and nothing else.\n",
    "\n",
    "Having identified it's believed present state, it predicts that the future will consist of the following event states:\n",
    "\n",
    "    'future': [['world']]\n",
    "\n",
    "That's just one more event consisting of the `'world'` symbol.\n",
    "\n",
    "\n",
    "Let's now complete this sequence for the agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe 2nd event in New Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'observed'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}}\n",
    "agent.observe(data=data,nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'confidence': 1,\n",
       "  'confluence': 0.46,\n",
       "  'emotives': {},\n",
       "  'entropy': 0.99315685,\n",
       "  'evidence': 1,\n",
       "  'extras': [],\n",
       "  'fragmentation': 0,\n",
       "  'frequency': 1,\n",
       "  'future': [],\n",
       "  'grand_hamiltonian': 0.496578425,\n",
       "  'hamiltonian': 0.5,\n",
       "  'itfdf_similarity': 1,\n",
       "  'matches': ['hello', 'world'],\n",
       "  'missing': [],\n",
       "  'name': '7d0678ba6305341ce0d25133ab086208656a562f',\n",
       "  'past': [],\n",
       "  'potential': 4,\n",
       "  'present': [['hello'], ['world']],\n",
       "  'similarity': 1,\n",
       "  'snr': 1,\n",
       "  'type': 'prototypical'},\n",
       " {'confidence': 1,\n",
       "  'confluence': 0.3,\n",
       "  'emotives': {},\n",
       "  'entropy': 0.528771222,\n",
       "  'evidence': 0.333333343,\n",
       "  'extras': ['hello'],\n",
       "  'fragmentation': 0,\n",
       "  'frequency': 1,\n",
       "  'future': [],\n",
       "  'grand_hamiltonian': 0.264385611,\n",
       "  'hamiltonian': 0,\n",
       "  'itfdf_similarity': 0.94721359,\n",
       "  'matches': ['world'],\n",
       "  'missing': [],\n",
       "  'name': '3b5c9cdc4424988308922d2ec8c7bc06b7c6ac21',\n",
       "  'past': [['goodbye'], ['cruel']],\n",
       "  'potential': 2.39165807,\n",
       "  'present': [['world']],\n",
       "  'similarity': 0.4,\n",
       "  'snr': 0.333333343,\n",
       "  'type': 'prototypical'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_predictions(nodes=query_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, 'P1' produced two predictions.  The first is still a prediction of \"hello world\", but now the prediction object's fields have changed.  It now believes the current present state to be:\n",
    "\n",
    "    'present': [['hello'], ['world']]\n",
    "\n",
    "based on its matches:\n",
    "\n",
    "    'matches': ['hello', 'world']\n",
    "\n",
    "It no longer expects to find anything in the future.\n",
    "\n",
    "    'future': []\n",
    "    \n",
    "However, it's second prediction object sees things a little differently.  It is based on the \"goodbye cruel world\" sequence.  Therefore, based on that sequence, it believes the present state to be:\n",
    "\n",
    "    'present': [['world']]\n",
    "    \n",
    "since it only matched on:\n",
    "\n",
    "    'matches': ['world']\n",
    "    \n",
    "Since this is its belief for the present state, it is surprised to find the symbol `'hello'` and shows this as an 'extra' item:\n",
    "\n",
    "    'extras': ['hello']\n",
    "    \n",
    "Additionally, based on its belief of the present state, it predicts backwards into the past and says that the events containing the symbols `'goodbye'` and `'cruel'` must have occurred although it did not get a chance to observe them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An Emotional Machine: Emotives and Mood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The emotives field in the GDF is used to give the agent emotional data. It is used for automatic goal-setting, reinforcement learning, and determining the desirability of an outcome. Examples of emotional data would be cost and time. In order for your agent to learn emotional data you have to populate the emotive field in the GDF with the name of the emotive, and then give it a value; emotives accept non-vector numbers, positive or negative.\n",
    "\n",
    "Furthermore, emotives can be used to help solve multivariate problems. Let's start by using one emotive, then later on in the tutorial we will show you how solve problems with multiple emotives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all-cleared'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.clear_all_memory(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_1 = [{\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {'utility': 10}}, \n",
    "              {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}}]\n",
    "\n",
    "sequence_2 = [{\"strings\": [\"goodbye\"], \"vectors\": [], \"emotives\": {'utility': -10}},\n",
    "              {\"strings\": [\"cruel\"], \"vectors\": [], \"emotives\": {'utility': -40}},\n",
    "              {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decide that the two sequences above are polar opposites in sentiment, and each have their own magnitude in value, we can encode that information as a \"emotives\" value in the GDF. In the above sequences we have named the emotive, \"utility\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For people, the two sequences provided above have very different emotional context around them.  \"hello world\" may be a happy or neutral sentiment.  \"goodbye cruel world\", however, has a very negative connotation to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the positioning of the emotives values within the sequence is irrelevant.  We could have just as easily put them in the last event of the sequence, or even created a new event just to supply a emotive value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_1 = [{\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {}}, \n",
    "              {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {'utility': 10}}]\n",
    "\n",
    "sequence_2 = [{\"strings\": [\"goodbye\"], \"vectors\": [], \"emotives\": {}},\n",
    "              {\"strings\": [\"cruel\"], \"vectors\": [], \"emotives\": {}},\n",
    "              {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {'utility': -50}}]\n",
    "\n",
    "# or\n",
    "\n",
    "sequence_1 = [{\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {}}, \n",
    "              {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}},\n",
    "              {\"strings\": [], \"vectors\": [], \"emotives\": {'utility': 10}}]\n",
    "\n",
    "sequence_2 = [{\"strings\": [\"goodbye\"], \"vectors\": [], \"emotives\": {}},\n",
    "              {\"strings\": [\"cruel\"], \"vectors\": [], \"emotives\": {}},\n",
    "              {\"strings\": [\"world\"], \"vectors\": [], \"emotives\": {}},\n",
    "              {\"strings\": [], \"vectors\": [], \"emotives\": {'utility': -50}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PREDICT': True,\n",
       " 'SLEEPING': False,\n",
       " 'emotives': {},\n",
       " 'last_learned_model_name': '',\n",
       " 'models_kb': '{KB| objects: 0}',\n",
       " 'name': 'P1',\n",
       " 'size_WM': 0,\n",
       " 'target': '',\n",
       " 'time': 0,\n",
       " 'vectors_kb': '{KB| objects: 0}'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MODEL|7d0678ba6305341ce0d25133ab086208656a562f'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learn the 1st sequence\n",
    "for event in sequence_1:\n",
    "    agent.observe(data=event,nodes=ingress_nodes)\n",
    "\n",
    "agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MODEL|3b5c9cdc4424988308922d2ec8c7bc06b7c6ac21'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learn the 2nd sequence\n",
    "for event in sequence_2:\n",
    "    agent.observe(data=event,nodes=ingress_nodes)\n",
    "\n",
    "agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's observe the single event with \"hello\" in the strings section to get a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'observed'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.observe({\"strings\": [\"hello\"], \"vectors\": [], \"emotives\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'confidence': 1,\n",
       "  'confluence': 0.4,\n",
       "  'emotives': {'utility': 10},\n",
       "  'entropy': 0.464385629,\n",
       "  'evidence': 0.5,\n",
       "  'extras': [],\n",
       "  'fragmentation': 0,\n",
       "  'frequency': 1,\n",
       "  'future': [['world']],\n",
       "  'grand_hamiltonian': 0.232192814,\n",
       "  'hamiltonian': 0,\n",
       "  'itfdf_similarity': 1,\n",
       "  'matches': ['hello'],\n",
       "  'missing': [],\n",
       "  'name': '7d0678ba6305341ce0d25133ab086208656a562f',\n",
       "  'past': [],\n",
       "  'potential': 3.5,\n",
       "  'present': [['hello']],\n",
       "  'similarity': 0.666666687,\n",
       "  'snr': 1,\n",
       "  'type': 'prototypical'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_predictions(nodes=query_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_predictions command gave back one sequence as a prediction.\n",
    "This sequence has an emotive with the name of \"utility\" of value 10 in its prediction object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utility': 10}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_predictions(nodes=query_nodes)[0]['emotives']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Emotives allow room for decision making based on emotions. Humans make decisions based on emotions all the time but the outcome depends on their mood. A user can set a mood for each emotive in order to give them weights, which will affect the overall utility. More on this later when we create a recommendation engine based on different emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Numerical Data: Vectors\n",
    "\n",
    "Gaius is able to convert numerical data, in the form of vectors, into symbols for pattern matching. For example, the hand written digits dataset is an example of numerical data where the pixel values will be converted into a flattened vector.\n",
    "\n",
    "But first let's talk about vectors and how to format them into a GDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <b>vector</b> is also an array. For example, [1, 1] is a vector. To provide one vector, the data will look like this:\n",
    "\n",
    "~~~\n",
    "{\"strings\": [], \"vectors\": [[1, 1]], \"emotives\": {}}\n",
    "~~~\n",
    "\n",
    "Notice the two sets of square brackets, [ ]. This is because we are putting an array inside another array. Vector basis values can be any real number, floats or integers.\n",
    "\n",
    "{\"vectors\": [[4.986, 100, 33.12, 33.91]], \"strings\": [], \"emotives\": {}}\n",
    "\n",
    "Multiple vectors with the same dimensions form a matrix, and can be processed as matrices by Manipulatives. (Great for image processing.):\n",
    "\n",
    "{\"vectors\": [[230, 188, 255], [125, 39, 64], [14, 138, 220]], \"strings\": [], \"emotives\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's grab the digits dataset from sklearn, and later view some of the digits with matplotlib\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "d = load_digits()\n",
    "dataset = []\n",
    "for i in range(len(d['data'])):\n",
    "    flattened_vector = d['data'][i].tolist()\n",
    "    label = d['target_names'] [d['target'][i]]\n",
    "    \n",
    "    \n",
    "    sequence = [{\"strings\": [], \"vectors\": [flattened_vector], \"emotives\": {}},\n",
    "                {\"strings\": [f'label|{label}'], \"vectors\": [], \"emotives\": {}}]\n",
    "    dataset.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's look at the first sequence\n",
    "# 1st event contains the flattened vector in the vectors field\n",
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd event is the label/answer in the strings field\n",
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's verify one of the image of the pixel data\n",
    "digit_img = dataset[0][0]['vectors'][0]\n",
    "digit_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view this '0' digit\n",
    "chunks_v = [digit_img[x:x+8] for x in range(0, len(digit_img), 8)]\n",
    "array_chunks = np.array(chunks_v).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(array_chunks, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset[0][1]['strings'][0]\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label is correct, the pixel data does represent the digit, '0'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have the agent learn this image using the vector field in the GDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fresh start\n",
    "agent.clear_all_memory(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data and label of the 1st digit\n",
    "event_1 = dataset[0][0]\n",
    "event_2 = dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the data\n",
    "agent.observe(data=event_1,nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what's inside the wm\n",
    "agent.get_wm(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in order to represent the vector as a symbol it has been converted into a hash to uniquely represent that vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed its label so we can teach the agent what it is\n",
    "agent.observe(data=event_2,nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_wm(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's teach it another digit\n",
    "event_1 = dataset[1][0]\n",
    "event_2 = dataset[1][1]\n",
    "agent.observe(data=event_1,nodes=ingress_nodes)\n",
    "agent.observe(data=event_2,nodes=ingress_nodes)\n",
    "agent.learn(nodes=ingress_nodes)\n",
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's observe a new digit to see if it can predict its label\n",
    "event_1 = dataset[11][0]  # data\n",
    "event_2 = dataset[11][1]  # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.observe(data=event_1,nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "event_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_digit_img = dataset[11][0]['vectors'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view this digit\n",
    "chunks_v = [new_digit_img[x:x+8] for x in range(0, len(new_digit_img), 8)]\n",
    "array_chunks = np.array(chunks_v)\n",
    "array_chunks_2 = array_chunks.astype('uint8')\n",
    "plt.imshow(array_chunks_2, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's see what the agent predicts this digit is based on the images it has learned\n",
    "agent.get_predictions(nodes=query_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore fields of the 1st prediction object from its prediction ensemble\n",
    "print(f\"prediction = {agent.get_predictions(nodes=query_nodes)[0]['future'][0][0]}\")\n",
    "print(f\"potential  = {agent.get_predictions(nodes=query_nodes)[0]['potential']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore fields of the 2nd prediction object from its prediction ensemble\n",
    "print(f\"prediction = {agent.get_predictions(nodes=query_nodes)[1]['future'][0][0]}\")\n",
    "print(f\"potential  = {agent.get_predictions(nodes=query_nodes)[1]['potential']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the agent has two sequences that were learned previously, the '0' and the '1', those are the only two things it can predict when observing new data. We extract the <b>future</b> field of the prediction object in order to get the \"prediction\". This is the reason why we observe the label/answer as the 2nd event of the sequences. The strength of the predictions are determined by its <b>potential</b> score.\n",
    "\n",
    "The first prediction that the agent produced gave the answer '1' with a potential of 2.5. The second prediction gave an answer '0' with a potential of ~1.95. Since the first prediction has a higher potential than the second prediction, the agent has determined that the answer is more likely to be '1', based on what's in its kb, which is correct in this case. Moreover, by default prediction ensembles are sorted from highest potential to lowest potential. It is up to the user on how they want to use the information provided in the prediction objects/ensembles.\n",
    "\n",
    "Questions you might want to answer when using the prediction ensembles:\n",
    "\n",
    "Do you want to use only the strongest prediction or do you want an average of all predictions?\n",
    "\n",
    "Do you want the agent to only give a maximum number of predictions of 10 or 100, which you can later use to get the average of predictions?\n",
    "\n",
    "Again, it is up to you on how you would like to use the prediction objects/ensemble. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's say you only care about the strongest prediction an agent might give from a particular node. In this case, just query the first prediction object, which is the strongest prediction because it has the highest potential value in the prediction ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1st prediction object from its prediction ensemble\n",
    "print(f\"prediction = {agent.get_predictions(nodes=query_nodes)[0]['future'][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Attaching Manipulatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Manipulatives</b> are small bits of code that operate on incoming or outgoing data. They are attached to a primitive. They can be wired together in series or parallel to achieve certain behavior for an agent. The way these are wired together is called the agents topology. We've usee only one primitive earlier but now will use primitives with manipulatives attached to them. But first let's understand that:\n",
    "\n",
    "Manipulatives <i>may</i> have configurable parameters called Genetic Configurable Parameters (GCP).\n",
    "\n",
    "Manipulatives come in 2 flavors:\n",
    "\n",
    "<i>manipulatives</i> ---\n",
    "These take incoming data, operate on it, and return the manipulated data. Incoming data may be streamed from other manipulatives, or from the observed API call.\n",
    "\n",
    "<i>abstraction manipulatives</i> --- manipulates data by filtering selected fields from the prediction object in order to be fed into another primitive in another symbolic form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine a scenario where you have two primitive nodes all being fed the same emotional data (emotives). You're interested in buffing up one of those primitives by multiplying its value, but you don't want to change the input data on the other primitive. You can create a topology where you can manipulate the data of one of those primitive nodes. Let's practice using the manipulative with that node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Manipulatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use different ingress and query nodes from the genome where there is a manipulative attached to a primitive. The manipulative we're going to attach is called, \"emotiveMultiplier.\"\n",
    "\n",
    "<i>To create this manually in the PVT simple go to Genie Creator, then find the manipulative named, \"emotiveMultiplier,\" in the 'Manipulatives' section, next click & drag the manipulative on top of the 'P1' primitive. An arrow from the manipulative node to the P1 primitive should appear. You can edit its GCP by right-clicking on the node, and then clicking on \"edit\"</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use primitive, P2, to demonstrate Manipulatives\n",
    "ingress_nodes = ['P2']\n",
    "query_nodes   = ['P2']\n",
    "\n",
    "agent.set_ingress_nodes(ingress_nodes)\n",
    "agent.set_query_nodes(query_nodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out all memory from all nodes just in case you were tinkering around.\n",
    "agent.clear_all_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.clear_wm(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an event with two symbols, hello and world, with a positive emotional value of 10 in its emotives field\n",
    "event = {\"strings\": ['hello','world'],\"vectors\": [],\"emotives\": {'utility': 10}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.observe(data=event,nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what's in the working memory. Although, the emotive/utility value is not present in the wm\n",
    "agent.get_wm(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the cognition data to see the emotive/utility value. Here the utility = 50, because 10*5=50\n",
    "agent.get_cognition_data(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.observe(data={\"strings\": ['hello','world'],\"vectors\": [],\"emotives\": {}},nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_predictions(nodes=query_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the learned sequence that it matched on originally was fed a emotive/utility value of 10, but because it had a emotiveMulitplier manipulative attached to it the utility for this sequence has been multiplied by 5 to give it a value of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_predictions(nodes=query_nodes)[0]['emotives']['utility']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Abstraction Manipulatives and Deep Learning\n",
    "\n",
    "Hierarchies are important structures in natural intelligence devices.  (Uh, I mean biological brains!) . Passing processed information from lower layers in a hierarchy for further processing in higher layers leads to interesting outputs such as language and higher-order reasoning.  Even in simulated models such as artificial neural networks and convolutional neural networks, hierarchical structures allow separation and extraction of progressively higher-order features from raw data.  What is the functional equivalent of this in our analytical information model?\n",
    "\n",
    "This is where GAIuS' abstraction manipulatives come in.\n",
    "\n",
    "Using abstractions provide some advantages in some use cases, increasing accuracy, precision, and/or other metrics. It is not the case that further processing always results in improved predictions. Some datasets may not have any additional information that can be squeezed out through further processing. Other datasets show clear improvements beyond the lower layers' predictions.\n",
    "\n",
    "Each abstraction manipulative isolates a single field from the prediction objects emitting from a cognitive processor, and passes that field's values to a cognitive processor in the next layer.\n",
    "\n",
    "The genome topology, 'demo-all', provided below for this portion of the tutorial includes a 2-layer hierarchy of nodes. Node P3 is the first layer. The second layer consists of which abstracted data feeds into P4. Each abstraction manipulative isolates a single field from the prediction objects emitting from a cognitive processor, and passes that field's values to a cognitive processor in the next layer.\n",
    "\n",
    "As of now, the available fields are:\n",
    "\n",
    "    name\n",
    "    matches\n",
    "    future\n",
    "    missing\n",
    "    extras\n",
    "    present\n",
    "    past\n",
    "\n",
    "Create a new genome where you attach an abstraction manipulative node from a lower node, 'P1', to a higher node, 'P2'. In the Genie Creator, 'click and drag' the abstraction manipulative from the 'Manipulative' section onto P1. An arrow coming from the node to the abstraction node should appear. Then, 'right-click and hold' on the white space in the topology section to select 'Add Primitive'. A new primitive node, 'P2', should appear. 'Right-click and hold' on the abstraction node, select 'Draw Edge From/To'. The abstraction node and the new primitive node will become highlighted. 'Right-click and hold' on the primitive node,'P2', select 'Draw Edge From/To'. An arrow from the abstraction node to P2 should appear. 'Click and drag' the nodes so that it will be align vertically. In order to abstract a particular field, 'right-click and hold' on the abstraction node to select 'Edit Properties'. In the value section, you can change which field you want to abstract. For now, let's leave it with the default field value, 'name', so that we can abstract the name of sequences learned. Spawn a new bottle with this genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use primitives P3 & P4\n",
    "ingress_nodes = [\"P3\",\"P4\"]\n",
    "query_nodes   = [\"P3\",\"P4\"]\n",
    "\n",
    "agent.set_ingress_nodes(ingress_nodes)\n",
    "agent.set_query_nodes(query_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the agent learned a sequence. let's see what happens when the agent observes a similar event that matches what it has in its kb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_1 = {\"strings\": ['hello','world'],\"vectors\": [],\"emotives\": {}}\n",
    "agent.observe(data=event_1,nodes=ingress_nodes)\n",
    "agent.get_wm(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the order of the symbols 'hello' and 'friend' in the strings section were reversed when it reached the wm. This is because there is a parameter in the genome called 'sort' which is turned on by default when you create a genome topology using the PVT. The 'sort' parameter sorts the strings symbols in alphabetical order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the primitive and the sequence matched,'PRIMITIVE|p3ed9a6f8e|name|3814b8c03c137909546d6993c0acf351981bccae', in its kb has been <b>abstracted</b> out and added to the wm of P4 as a symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_2 = {\"strings\": ['hello','friend'],\"vectors\": [],\"emotives\": {}}\n",
    "agent.observe(data=event_2,nodes=ingress_nodes)\n",
    "agent.get_wm(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_predictions(nodes=query_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Gaius Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of tests that are common to run: <b>Classification</b>, <b>Utility Polarity</b>, and <b>Utility Value</b>. Depending on the nature of dataset and what the user is trying to get out of the dataset determines which type of test to run. Below are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the first primitive for our ingress and query node\n",
    "ingress_nodes = ['P1']\n",
    "query_nodes   = ['P1']\n",
    "\n",
    "agent.set_ingress_nodes(ingress_nodes)\n",
    "agent.set_query_nodes(query_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification predictions\n",
    "\n",
    "Classification problems are problems where sequences include some context (vectors, strings, or mixed), and a specific label string is required to identify that context.\n",
    "\n",
    "For example, a record from the \"iris\" dataset consists of sequence with a vector in the first event, and a label as a string in the second event:\n",
    "\n",
    "~~~\n",
    "event_1_data = {\"strings\": [], \"vectors\": [[5.1, 3.5, 1.4, 0.2]], \"emotives\": {}}\n",
    "event_2_data = {\"strings\": [\"setosa\"], \"vectors\": [], \"emotives\": {}}\n",
    "~~~\n",
    "\n",
    "The above two events comprise a single sequence.\n",
    "\n",
    "Classification problems are easily handled by an agent.  Best-practice is to provide the classification as its own event at the end of a sequence.  This way, the classification can always be extracted from a prediction by querying the last event in the 'future' field.  All the prior events provide the contextual data for the classification.  This can be one or more events containing strings and/or vectors.  Therefore, your sequence for classification problems must contain at least two events during training.\n",
    "\n",
    "For \"classification\" problems, you will remove the last symbol from the last event of the \"strings\" field in a sequence during the testing phase. Once the answer-free data is ingested (via \"observe\" API calls), the system is queried (via \"getPredictions\" API calls) for its predictions.  If the prediction matches the historic answer, then the test marks the prediction as correct.  If not, then it is marked as incorrect.  After the predictions are returned and tallied, the correct answer will be provided to the agent's *query* nodes  as its own event containing the classification in the \"strings\" field to continue learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way this works is that the historic data must include the answers in order to measure the performance of the agent. Each of the sequences are independent of each other. If using files for the datasets, then each sequence must be in its own file. Each row in that file must contain a JSON dump of the GDF object. Each file must contain only one full sequence.  All of these sequence files must be placed in a single directory.  \n",
    "\n",
    "For example, this will convert the iris dataset into files stored in a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import json\n",
    "import os\n",
    "\n",
    "# get iris dataset from sklearn\n",
    "iris_data = load_iris()\n",
    "\n",
    "# define paths to create/use\n",
    "datasets_file_path     = '../data/gdf_folder/testset'\n",
    "iris_dataset_file_path = '../data/gdf_folder/testset/iris-flowers'\n",
    "\n",
    "# check to see if path exist, if not, then create it\n",
    "if not os.path.exists(datasets_file_path):\n",
    "    os.mkdir(datasets_file_path)\n",
    "    \n",
    "# check to see if path exist, if not, then create it\n",
    "if not os.path.exists(iris_dataset_file_path):\n",
    "    os.mkdir(iris_dataset_file_path)    \n",
    "\n",
    "# iterate through testset to create a folder with sequences for training and testing purposes\n",
    "# every file will have two events. The first event will have descriptive data while the second event will have the label\n",
    "for i in range(len(iris_data['data'])):\n",
    "    \n",
    "    iris_vector_data = iris_data['data'][i].tolist()\n",
    "    iris_label = iris_data['target_names'] [iris_data['target'][i]]\n",
    "    \n",
    "    sequence = [{\"strings\": [], \"vectors\": [iris_vector_data], \"emotives\": {}},\n",
    "                {\"strings\": [f'label|{iris_label}'], \"vectors\": [], \"emotives\": {}}]\n",
    "    \n",
    "    with open(f'{iris_dataset_file_path}/{iris_label}-{i}', 'w') as f:\n",
    "        f.write(f'{json.dumps(sequence[0])}\\n')\n",
    "        f.write(f'{json.dumps(sequence[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Gene Parameters of Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we were able to change the gene parameters of the Genome before we spawned an agent. However, you may want to change them afterwards. For the Iris dataset we are going to decrease the recall threshold gene parameter to increase the response rate when making a prediction. Again, 'recall_threshold' is the minimum likeness of the observed sequence against known sequences for it to be returned as a prediction. The smaller the threshold, the less the observed needs to match the known. The higher the threshold, the more similar the known must be to the observed for it to be returned as a prediction. Values 0 < x <= 1. Typically between 0.2-0.8. Let's increase max_predictions 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check recall threshold of P1 with get_gene method\n",
    "for prim in ingress_nodes:\n",
    "    if prim == 'P1':\n",
    "        print(agent.get_gene(\"recall_threshold\", prim), agent.get_gene(\"max_predictions\", prim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the recall threshold and max predictions gene\n",
    "for prim in ingress_nodes:\n",
    "    if prim == 'P1':\n",
    "        agent.change_genes({\"recall_threshold\": 0.1}, [prim])\n",
    "        agent.change_genes({\"max_predictions\": 10}, [prim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check changed recall threshold of P1 with get_gene method\n",
    "for prim in ingress_nodes:\n",
    "    if prim == 'P1':\n",
    "        print(agent.get_gene(\"recall_threshold\", prim), agent.get_gene(\"max_predictions\", prim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the gaius data_ops library to split the data into training and testing sets\n",
    "from ia.gaius import data_ops\n",
    "\n",
    "iris_dataset = data_ops.Data(data_directories=[iris_dataset_file_path])\n",
    "iris_dataset.prep(percent_of_dataset_chosen=100, percent_reserved_for_training=80, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset.train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset.test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iris_dataset.train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iris_dataset.test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset.train_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset.test_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.clear_all_memory(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Remember when training sequences to agents to keep the convention of clear_wm --> observe events--> learn sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if bottle has no data in kb\n",
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Agent\n",
    "for i, file_path in enumerate(iris_dataset.train_sequences):\n",
    "\n",
    "    with open(iris_dataset.train_sequences[i], \"r\") as sequence_file:        \n",
    "        \n",
    "        sequence = sequence_file.readlines()\n",
    "        sequence = [json.loads(d) for d in sequence]\n",
    "        \n",
    "        for event in sequence:\n",
    "        \n",
    "            agent.observe(data=event,nodes=ingress_nodes)\n",
    "            \n",
    "    agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show status\n",
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the models_kb has been updated. For vector data the agent keeps track of vectors separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, clear_wm --> observe --> get_predictions --> learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Agent\n",
    "iris_predictions = []\n",
    "iris_actuals     = []\n",
    "\n",
    "for i, file_path in enumerate(iris_dataset.test_sequences):\n",
    "\n",
    "    with open(iris_dataset.test_sequences[i], \"r\") as sequence_file:\n",
    "        \n",
    "        agent.clear_wm(nodes=ingress_nodes)\n",
    "        \n",
    "        sequence = sequence_file.readlines()\n",
    "        sequence = [json.loads(d) for d in sequence]\n",
    "\n",
    "        # observe up to last event, which has the answer\n",
    "        for event in sequence[:-1]:\n",
    "        \n",
    "            agent.observe(data=event,nodes=ingress_nodes)\n",
    "            \n",
    "        # get and store predictions after observing events\n",
    "        iris_predictions.append(agent.get_predictions(nodes=query_nodes))\n",
    "\n",
    "        # store answers in a separate list for evaluation\n",
    "        iris_actuals.append(sequence[-1])\n",
    "\n",
    "        # observe answer\n",
    "        agent.observe(sequence[-1])\n",
    "\n",
    "        # learn answer (optional continous learning)\n",
    "        agent.learn(nodes=ingress_nodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iris_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction object gives an ensemble of predictions based on the max prediction gene parameter.\n",
    "The predictions are sorted by the potential score, strength of prediction, by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the first prediction ensemble with pretty print\n",
    "import pprint as pprint\n",
    "pprint.pprint(iris_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diving into this list of dictionaries we can get the amount of predictions given by P1\n",
    "len(iris_predictions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a classification problem we will set the label to be the second event of every sequence so that we can query out the future field of the prediction object. For example, let's look at the top two predictions for the first prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top prediction\n",
    "iris_predictions[0][0]['future'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The potential score for the top prediction\n",
    "iris_predictions[0][0]['potential']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second strongest prediction\n",
    "iris_predictions[0][1]['future'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The potential score for the second strongest prediction\n",
    "iris_predictions[0][1]['potential']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both predictions state that 'versicolor' is the answer based on sequences it learned prior. To get the name of the sequence that it matched on just query the 'name' field. This is useful when trying to explain why the agent gave a particular answer, hence, Explainable Artificial Intelligence (EAI). For instance,:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_predictions[0][0]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A user can use all of the predictions to tally them up to get an overall score or a user can simply just use the strongest prediction.\n",
    "\n",
    "For this tutorial we will only use the top predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how well the agent scores based on the top predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris_top_predictions = []\n",
    "iris_answers = []\n",
    "\n",
    "for i in range(0,len(iris_predictions)):\n",
    "    \n",
    "    try:\n",
    "        iris_top_predictions.append(iris_predictions[i][0]['future'][0][0])\n",
    "        \n",
    "    except:\n",
    "        # if the agent doesn't have enough information to make a prediction it wouldn't give one\n",
    "        iris_top_predictions.append('Unknown')\n",
    "        \n",
    "    iris_answers.append(iris_actuals[i]['strings'][0])\n",
    "    \n",
    "print(f'Accuracy = {round(accuracy_score(iris_answers,iris_top_predictions),2)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the top predictions\n",
    "iris_top_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the answers\n",
    "iris_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Polarity predictions\n",
    "\n",
    "Sometimes, all you want is to know if the output is positive or negative, true or false, yes or no.  For example, on customer churn data sets where a customers profile data of staying/leaving is converted with a +100/-100 emotive value, respectively, you simply want to know if the customer will leave or not. You may also want to have a probability associated with this. For testing purposes, though, we need an absolute answer rather than probabilities.  So, if the system predicts a negative utility regardless of the magnitude, then it is considered a prediction that the customer will churn.\n",
    "\n",
    "For \"utility polarity\" problems, you will remove and store the values from \"emotives\" during testing prior to sending to the ingress nodes. (Obviously, it will not remove those answers during training.) Once the answer-free data is ingested (via \"observe\" API calls), the system is queried (via \"getPredictions\" API calls) for its predictions.  If the prediction matches the historic answer, then the test marks the prediction as correct.  If not, then it is marked as incorrect. After the predictions are returned and tallied, the correct answer will be provided to the agent as its own event containing the utility in the \"emotives\" field to continue learning.  Note that if a sequence contains emotives in multiple events, all of them will be stripped of their emotives values and the sum will be used as the historic answer.\n",
    "\n",
    "For each prediction, the \"utility polarity\" converts the emotive \"utility\" value field into a positive/negative classification for tallying.\n",
    "\n",
    "Let's practice training and testing an agent from a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "churn_csv_file_path = '../data/churn.csv'\n",
    "df = pd.read_csv(churn_csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a glance of what the data looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are only two values for this column\n",
    "df['Churn?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing sets from pandas dataframe\n",
    "percentage_split = 0.1\n",
    "size      = len(df)\n",
    "train_num = int(size * percentage_split)\n",
    "\n",
    "training_df = df[:train_num]\n",
    "testing_df  = df[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all feature columns to use in a list\n",
    "# Excluding the phone columns because phone numbers are unique values which doesn't provide any useful data for pattern matching\n",
    "# Excluding the churn column because that is what we'll use for the answer.\n",
    "feature_columns = list(df.drop(columns=['Phone','Churn?'],axis=1))\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_for_output = ['Churn?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.clear_all_memory(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show status\n",
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert and split training and testing row data into a list of dictionaries\n",
    "train_list_of_dicts_from_rows = training_df.to_dict(\"records\")\n",
    "test_list_of_dicts_from_rows = testing_df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, we will use the | Pipe Character to categorize certain features with an associated value. This is helpful when running an information analysis on the dataset. This helps give insights on the data by determining key indicators and helping to separate signal from noise. For example, a customer's state location might be useful when pattern matching. Let's say a customer lives in NY. We will convert that to 'State|NY' so that if another customer was from NY there would possible be some correletion between the two. But don't worry about that. Let Gaius connect the dots for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You can supply multiple layers of categories when using the pipe character. For example, this GDF:\n",
    "\n",
    "{\"vectors\": [], \"strings\": [\"fruit|apple\", \"fruit|banana\", \"vegetable|celery\"], \"emotives\": {}}\n",
    "\n",
    "can be expanded to:\n",
    "    \n",
    "{\"vectors\": [], \"strings\": [\"food|fruit|apple\", \"food|fruit|banana\", \"food|vegetable|celery\"], \"emotives\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and modify data to insert into appropriate gdf sequence lists\n",
    "for index, dict_data in enumerate(train_list_of_dicts_from_rows):\n",
    "    \n",
    "    # make sure nothing is in wm before observing new data\n",
    "    agent.clear_wm(nodes=ingress_nodes)\n",
    "    \n",
    "    # reset variables\n",
    "    profile_data_piped_strings = []\n",
    "    churn_polarity_value = []\n",
    "    \n",
    "    # get key and values from each row to make profile data using the pipe character\n",
    "    for key, value in dict_data.items():\n",
    "        \n",
    "        if key in feature_columns:\n",
    "            \n",
    "            profile_data_piped_strings.append(f'{key}|{value}')            \n",
    "            \n",
    "        elif key in column_for_output:\n",
    "            \n",
    "            # converting true and false to positive and negative values to insert into emotives utility field\n",
    "            if value == 'True.':\n",
    "                value = -100\n",
    "            else:\n",
    "                value = 100\n",
    "                \n",
    "            churn_polarity_value.append(int(f'{value}'))\n",
    "    \n",
    "    # convert data to gdf\n",
    "    profile_data_gdf   = {\"vectors\":[], \"strings\": profile_data_piped_strings, \"emotives\": {}}\n",
    "    churn_polarity_gdf = {\"vectors\":[], \"strings\": [], \"emotives\": {\"utility\": churn_polarity_value[0]}}\n",
    "    \n",
    "    # observe events, and then learn sequence\n",
    "    agent.observe(data=profile_data_gdf,nodes=ingress_nodes)\n",
    "    agent.observe(data=churn_polarity_gdf,nodes=ingress_nodes)\n",
    "    agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_predictions = []\n",
    "churn_actuals = []\n",
    "\n",
    "# get and modify data to insert into appropriate gdf sequence lists\n",
    "for index, dict_data in enumerate(test_list_of_dicts_from_rows[:10]):\n",
    "    \n",
    "    # make sure nothing is in wm before observing new data\n",
    "    agent.clear_wm(nodes=ingress_nodes)\n",
    "    \n",
    "    # reset variables\n",
    "    profile_data_piped_strings = []\n",
    "    churn_polarity_value = []\n",
    "    \n",
    "    # get key and values from each row to make profile data using the pipe character\n",
    "    for key, value in dict_data.items():\n",
    "        \n",
    "        if key in feature_columns:\n",
    "            \n",
    "            profile_data_piped_strings.append(f'{key}|{value}')            \n",
    "            \n",
    "        elif key in column_for_output:\n",
    "            \n",
    "            # converting true and false to positive and negative values to insert into emotives utility field\n",
    "            if value == 'True.':\n",
    "                value = -100\n",
    "            else:\n",
    "                value = 100\n",
    "                \n",
    "            churn_polarity_value.append(int(f'{value}'))\n",
    "    \n",
    "    # convert data to gdf\n",
    "    profile_data_gdf   = {\"vectors\":[], \"strings\": profile_data_piped_strings, \"emotives\": {}}\n",
    "    churn_polarity_gdf = {\"vectors\":[], \"strings\": [], \"emotives\": {\"utility\": churn_polarity_value[0]}}\n",
    "    \n",
    "    # observe events, and then learn sequence\n",
    "    agent.observe(data=profile_data_gdf,nodes=ingress_nodes)\n",
    "    \n",
    "    # get all predictions into a list\n",
    "    churn_predictions.append(agent.get_predictions(nodes=query_nodes))\n",
    "    \n",
    "#     get all historical answers for later analysis\n",
    "    churn_actuals.append(churn_polarity_value[0])    \n",
    "    \n",
    "#     learn the answer\n",
    "    agent.observe(data=churn_polarity_gdf,nodes=ingress_nodes)\n",
    "    agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show status\n",
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time instead of extracting the 'future' field from the prediction object we will extract the 'emotives' field, instead. For example, let's look at the top prediction of the first prediction,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_predictions[0][0]['emotives']['utility']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above utility score means that the customer stayed, 'False.', which we converted +100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again we are going to use the top predictions only\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "churn_top_predictions = []\n",
    "\n",
    "for i in range(0,len(churn_predictions)):\n",
    "    \n",
    "    try:\n",
    "        churn_top_predictions.append(churn_predictions[0][0]['emotives']['utility'])\n",
    "        \n",
    "    except:\n",
    "        # if the agent doesn't have enough information to make a prediction it wouldn't give one\n",
    "        churn_top_predictions.append('Unknown')\n",
    "    \n",
    "print(f'Accuracy = {round(accuracy_score(churn_actuals,churn_top_predictions),2)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Value predictions\n",
    "\n",
    "Other times, you need to predict actual numeric values.  For example, in the Boston housing market data sets, the goal is to predict house prices.  These can be recorded as emotive values.  It is obviously rare to have negative house prices, so the polarity report wouldn't be helpful.  Instead, we're interested in predicting the actual market price of each house.\n",
    "\n",
    "This problem type works identically to the one above, except it retains the numeric value at the end instead of converting it into a positive/negative classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fresh start\n",
    "agent.clear_all_memory(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import json\n",
    "import os\n",
    "\n",
    "# get data\n",
    "boston = load_boston(True)\n",
    "\n",
    "datasets_file_path     = '../data/gdf_folder/testset'\n",
    "bhp_dataset_file_path  = '../data/gdf_folder/testset/boston_house_prices'\n",
    "\n",
    "# check to see if path exist, if not, then create it\n",
    "if not os.path.exists(datasets_file_path):\n",
    "    os.mkdir(datasets_file_path)\n",
    "    \n",
    "# check to see if path exist, if not, then create it\n",
    "if not os.path.exists(bhp_dataset_file_path):\n",
    "    os.mkdir(bhp_dataset_file_path)\n",
    "\n",
    "numberofrecord = boston[1].shape[0]\n",
    "data =  boston[0]\n",
    "target = boston[1]\n",
    "\n",
    "features = ['CRIM', 'ZN','INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "\n",
    "for i, d, t in zip(range(1,numberofrecord+1),data, target):\n",
    "    \n",
    "    \n",
    "    profile_data_piped_strings = [ (f\"{var}|{str(float(item))}\") for var,item in zip(features, d) ]\n",
    "    house_price = int(t)\n",
    "    \n",
    "    sequence = [{'strings': profile_data_piped_strings, 'vectors': [], 'emotives': {}},\n",
    "                {'strings': [], 'vectors': [], 'emotives': {\"utility\": house_price}}] \n",
    "    \n",
    "    with open(f\"{bhp_dataset_file_path}/{i}\", 'w') as f:\n",
    "        f.write(f'{json.dumps(sequence[0])}\\n')\n",
    "        f.write(f'{json.dumps(sequence[1])}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the gaius data_ops library to split the data into training and testing sets\n",
    "from ia.gaius import data_ops\n",
    "\n",
    "bhp_dataset = data_ops.Data(data_directories=[bhp_dataset_file_path])\n",
    "bhp_dataset.prep(percent_of_dataset_chosen=100, percent_reserved_for_training=80, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhp_dataset.train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bhp_dataset.test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bhp_dataset.train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bhp_dataset.test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhp_dataset.train_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhp_dataset.test_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if bottle has no data in kb\n",
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Agent\n",
    "for i, file_path in enumerate(bhp_dataset.train_sequences):\n",
    "\n",
    "    with open(bhp_dataset.train_sequences[i], \"r\") as sequence_file:        \n",
    "        \n",
    "        sequence = sequence_file.readlines()\n",
    "        sequence = [json.loads(d) for d in sequence]\n",
    "        \n",
    "        for event in sequence:\n",
    "        \n",
    "            agent.observe(data=event,nodes=ingress_nodes)\n",
    "            \n",
    "    agent.learn(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show status\n",
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the models_kb has been updated. For vector data the agent keeps track of vectors separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, clear_wm --> observe --> get_predictions --> learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Agent\n",
    "bhp_predictions = []\n",
    "bhp_actuals = []\n",
    "\n",
    "for i, file_path in enumerate(bhp_dataset.test_sequences):\n",
    "\n",
    "    with open(bhp_dataset.test_sequences[i], \"r\") as sequence_file:\n",
    "        \n",
    "        agent.clear_wm(nodes=ingress_nodes)\n",
    "        \n",
    "        sequence = sequence_file.readlines()\n",
    "        sequence = [json.loads(d) for d in sequence]\n",
    "\n",
    "        # observe up to last event, which has the answer\n",
    "        for event in sequence[:-1]:\n",
    "        \n",
    "            agent.observe(data=event,nodes=ingress_nodes)\n",
    "            \n",
    "        # get and store predictions after observing events\n",
    "        bhp_predictions.append(agent.get_predictions(nodes=query_nodes))\n",
    "\n",
    "        # store answers in a separate list for evaluation\n",
    "        bhp_actuals.append(sequence[-1])\n",
    "\n",
    "        # observe answer\n",
    "        agent.observe(data=sequence[-1],nodes=ingress_nodes)\n",
    "\n",
    "        # learn answer (optional continous learning)\n",
    "        agent.learn(nodes=ingress_nodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bhp_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction object gives an ensemble of predictions based on the max prediction gene parameter.\n",
    "The predictions are sorted by the potential score, strength of prediction, by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the first prediction ensemble with pretty print\n",
    "import pprint as pprint\n",
    "pprint.pprint(bhp_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diving into this list of dictionaries we can get the amount of predictions given by P1\n",
    "len(bhp_predictions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top prediction\n",
    "bhp_predictions[0][0]['emotives']['utility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The potential score for the top prediction\n",
    "bhp_predictions[0][0]['potential']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second strongest prediction\n",
    "bhp_predictions[0][1]['emotives']['utility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The potential score for the second strongest prediction\n",
    "bhp_predictions[0][1]['potential']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhp_predictions[0][0]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A user can use all of the predictions to tally them up to get an overall score or a user can simply just use the strongest prediction.\n",
    "\n",
    "For this tutorial we will only use the top predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhp_actuals[0]['emotives']['utility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "bhp_top_predictions = []\n",
    "bhp_answers = []\n",
    "\n",
    "for i in range(0,len(bhp_predictions)):\n",
    "    \n",
    "    try:\n",
    "        bhp_top_predictions.append(bhp_predictions[i][0]['emotives']['utility'])   \n",
    "    except:\n",
    "        # depending on recall threshold and dataset agents may not have enough information to make a prediction;\n",
    "        # and, therefore will not provide one. In order to get certain metric evaluations it is best to replace non-predictions\n",
    "        # For this test we will replace non-predictions with nan values, but this is up to the user.\n",
    "        # Agents will either give a prediction or it won't. It is up to the user on how they want to use that information.        \n",
    "        bhp_top_predictions.append(np.nan)\n",
    "    \n",
    "    bhp_answers.append(bhp_actuals[i]['emotives']['utility'])\n",
    "\n",
    "# calculate errors removing unknown predictions\n",
    "bhp_errors = [x-y for x, y in zip(bhp_top_predictions, bhp_answers) if (math.isnan(x) == False)] \n",
    "    \n",
    "# It is often a good idea to see what percentage was the Agent able to make predictions.\n",
    "# For example, out of 100 tests 60 of them Agent was able to give predictions\n",
    "# Therefore, that would give a response rate of 60%\n",
    "\n",
    "# calculate response rate percentage\n",
    "orig_pred_length      = len(bhp_predictions)\n",
    "results_preds_updated = [x for x in bhp_top_predictions if (math.isnan(x) == False)]\n",
    "updated_pred_length   = len(results_preds_updated)\n",
    "response_rate         = np.round(updated_pred_length / orig_pred_length, 2) * 100\n",
    "\n",
    "# calculate symmetric mean absolute percentage error (sMAPE), an accuracy metric\n",
    "smape = 1.0/len(bhp_answers) * np.nansum(np.abs(np.array(bhp_answers) - np.array(bhp_top_predictions)) / (np.abs(bhp_answers) + np.abs(bhp_top_predictions)))\n",
    "\n",
    "# calculate root mean squared error (rmse), another accuracy metric\n",
    "bhp_errors_squared      = [x**2 for x in bhp_errors]\n",
    "bhp_errors_mean         = mean(bhp_errors_squared)\n",
    "root_mean_squared_error = np.sqrt(bhp_errors_mean)\n",
    "\n",
    "print(f'Response Rate  = {round(response_rate)}%\\n')\n",
    "print(f'sMAPE Accuracy = {round(1-smape, 2)*100}%\\n')\n",
    "print(f'RMSE           = {round(root_mean_squared_error)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Advanced Gaius Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10a. Building a Recommendation Engine with Emotives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Auto Miles Per Gallon (MPG) Dataset to demonstrate predicting two conflicting variables. Conflicting emotives would be 2 or more emotives that are not independent or mutually exclusive of each other. Meaning, they are inversely correlated to each other. A higher (positive) value for one is a lower (negative) value for the other. Horsepower and MPG are inversely related. We will use emotives to predict these two dependant variables, and then use the 'missing' field in the prediction object to provide recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from ia.gaius.agent_client import AgentClient\n",
    "from ia.gaius.prediction_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gdf(strings=None, vectors=None, emotives=None):\n",
    "    \"\"\"\n",
    "    Create GDF using supplied list of strings, vectors, and/or emotives\n",
    "    \"\"\"\n",
    "    gdf = {\n",
    "        \"vectors\": [] if vectors is None else vectors,\n",
    "        \"strings\": [] if strings is None else strings,\n",
    "        \"emotives\": {} if emotives is None else emotives\n",
    "    }\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    \"\"\"\n",
    "    A nice little Jupyter progress bar widget from: https://github.com/alexanderkuk/log-progress\n",
    "    \"\"\"\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )\n",
    "        \n",
    "\n",
    "def train_genie(agent, dataset, fresh_start, ingress_nodes):\n",
    "    \"\"\"\n",
    "    Train a bottle with the given dataset using the simple algorithm for\n",
    "    each sequence:\n",
    "        clearWM -> observe -> learn\n",
    "    \"\"\"\n",
    "    if fresh_start:\n",
    "        agent.clear_all_memory(nodes=ingress_nodes)\n",
    "    else:\n",
    "        agent.clear_wm(nodes=ingress_nodes)\n",
    "        \n",
    "    # Train Agent\n",
    "    for i, file_path in enumerate(log_progress(dataset, name = \"Training sequence\")):\n",
    "\n",
    "        with open(dataset[i], \"r\") as sequence_file:        \n",
    "\n",
    "            sequence = sequence_file.readlines()\n",
    "            sequence = [json.loads(d) for d in sequence]\n",
    "            \n",
    "            for event in sequence:\n",
    "\n",
    "                agent.observe(data=event,nodes=ingress_nodes)\n",
    "\n",
    "        agent.learn(nodes=ingress_nodes)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Information\n",
    "Source:\n",
    "\n",
    "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The dataset was used in the 1983 American Statistical Association Exposition.\n",
    "\n",
    "\n",
    "Data Set Information:\n",
    "\n",
    "This dataset is a slightly modified version of the dataset provided in the StatLib library. In line with the use by Ross Quinlan (1993) in predicting the attribute \"mpg\", 8 of the original instances were removed because they had unknown values for the \"mpg\" attribute. The original dataset is available in the file \"auto-mpg.data-original\". \n",
    "\n",
    "\"The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\" (Quinlan, 1993)\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. mpg: continuous \n",
    "2. cylinders: multi-valued discrete \n",
    "3. displacement: continuous \n",
    "4. horsepower: continuous \n",
    "5. weight: continuous \n",
    "6. acceleration: continuous \n",
    "7. model year: multi-valued discrete \n",
    "8. origin: multi-valued discrete \n",
    "9. car name: string (unique for each instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build a sample <b>recommendation</b> engine/layer using conflicting emotives. We're going to train the agent on automobile fuel efficiency (mpg) data. Then, we are going to give the agent two emotives, horsepower and mpg. At certain times we're going to change moods in order for the agent to recommend a car based on certain emotions, i.e. what you care about. For example, after the agent has certain knowledge about cars and their specs, what would the agent recommend if the user favored more about specific specs over other specs?\n",
    "\n",
    "This example will be in the form of a scenario where the user has built a recommendation engine. We're going to ask the agent for recommendations based on input data and what mood you're in.\n",
    "\n",
    "The agent will pattern match on sequences that you've trained it on. After observing input data the agent will attempt to provide predictions for the user. We will use the 'missing' field from the prediction object in order to provide recommendations. We will use the 'missing' field because the agent, after observing the data, matches on sequences that are in its knowledge base. The 'missing' field shows what the agent expected to be in an event of a particular sequence in its kb but isn't there. That is how we can use the 'missing' field to provide recommendations!\n",
    "\n",
    "Let's imagine that a user wanted a recommendation of cars that has <b>4 cylinders</b> and was manufactured in the year <b>1982</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "auto_mpg_df = pd.read_csv('../data/auto-mpg.csv.xls')\n",
    "auto_mpg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatype of dataset\n",
    "auto_mpg_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice horsepower datatype is 'object' which means there might be a discrepancy in the data. Let's explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_df.horsepower.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a string '?' in the data, so let's remove these from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_df = auto_mpg_df[auto_mpg_df.horsepower != '?']\n",
    "print('?' in auto_mpg_df.horsepower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_df.horsepower = auto_mpg_df.horsepower.astype('float')\n",
    "auto_mpg_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature columns\n",
    "auto_mpg_predictive_columns = ['mpg', 'horsepower']\n",
    "auto_mpg_unique_id = ['car name']\n",
    "auto_mpg_feature_columns = list(auto_mpg_df.columns.drop(auto_mpg_predictive_columns))\n",
    "auto_mpg_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_list_of_dicts_from_rows = auto_mpg_df.to_dict(\"records\")\n",
    "len(auto_mpg_list_of_dicts_from_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from dataframe to plot graph\n",
    "mpg_data = auto_mpg_df['mpg']\n",
    "horsepower_data = auto_mpg_df['horsepower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot inverse relation of mpg vs horsepower\n",
    "x = mpg_data\n",
    "y = horsepower_data\n",
    "plt.scatter(x, y, c='blue', alpha=0.5)\n",
    "plt.title(\"MPG vs Horspower\")          # title\n",
    "plt.xlabel(\"MPG\")                      # x label\n",
    "plt.ylabel(\"Horsepower\")               # y label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where you will put all sequences\n",
    "HP_MPG_GDF_DUMP_PATH                = '../data/gdf_folder/hp_mpg_gdfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mpg data into an array\n",
    "arr_mpg_data = np.array(mpg_data)\n",
    "arr_mpg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize mpg data\n",
    "normalized_mpg_data = preprocessing.normalize([arr_mpg_data],norm='l2')\n",
    "len(normalized_mpg_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalized_mpg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert hp data into an array, and then normalize\n",
    "arr_hp_data = np.array(horsepower_data)\n",
    "normalized_hp_data = preprocessing.normalize([arr_hp_data],norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalized_hp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot inverse relation of mpg vs horsepower\n",
    "x = normalized_mpg_data\n",
    "y = normalized_hp_data\n",
    "plt.scatter(x, y, c='blue', alpha=0.5)\n",
    "plt.title(\"MPG vs Horspower\")          # title\n",
    "plt.xlabel(\"MPG\")                      # x label\n",
    "plt.ylabel(\"Horsepower\")               # y label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update dataframe with normalized mpg and horsepower arrays\n",
    "auto_mpg_df['mpg'] = normalized_mpg_data[0]\n",
    "auto_mpg_df['horsepower'] = normalized_hp_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_list_of_dicts_from_rows = auto_mpg_df.to_dict(\"records\")\n",
    "len(auto_mpg_list_of_dicts_from_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Emotive GDFs for Horsepower and MPG Loads Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if path exist, if not, then create it\n",
    "if not os.path.exists(HP_MPG_GDF_DUMP_PATH):\n",
    "    os.mkdir(HP_MPG_GDF_DUMP_PATH)\n",
    "\n",
    "for index, dict_data in enumerate(auto_mpg_list_of_dicts_from_rows):\n",
    "    \n",
    "    # reset variables\n",
    "    profile_data = []\n",
    "    horsepower   = 0\n",
    "    mpg          = 0\n",
    "    car_name     = ''\n",
    "    \n",
    "    for key, value in dict_data.items():\n",
    "        \n",
    "        if key in auto_mpg_feature_columns:\n",
    "            \n",
    "            profile_data.append(f'{key}|{value}')\n",
    "            \n",
    "            if key in auto_mpg_unique_id:\n",
    "                \n",
    "                # we'll use the name of the vehicle to name the files\n",
    "                car_name = value\n",
    "                car_name = \"\".join(ch for ch in value if ch.isalnum() or ch.isspace()).replace(\" \",\"_\")\n",
    "            \n",
    "        elif key in auto_mpg_predictive_columns[0]:\n",
    "\n",
    "            horsepower = float(value)\n",
    "            \n",
    "        elif key in auto_mpg_predictive_columns[1]:\n",
    "            \n",
    "            mpg = float(value)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print(f'investigate {index}-{key}-{value}')\n",
    "    \n",
    "    # create gdf\n",
    "    profile_data_event = create_gdf(strings=profile_data,emotives={'horsepower': horsepower, 'mpg': mpg})\n",
    "    \n",
    "    # create sequence files\n",
    "    with open(f\"{HP_MPG_GDF_DUMP_PATH}/{index}_{car_name}\", 'w') as f:\n",
    "        json.dump(profile_data_event, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_dataset = data_ops.Data(data_directories=[HP_MPG_GDF_DUMP_PATH])\n",
    "auto_mpg_dataset.prep(percent_of_dataset_chosen=100, percent_reserved_for_training=80, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: When building a recommendation engines/layers with an agent it doesn't make sense to create a testing set because recommendations are evaluated by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genie(agent=agent,dataset=auto_mpg_dataset.train_sequences,fresh_start=True,ingress_nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.show_status(nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's change the recall threshold to 0.3 to increase the minimum likelihood of matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_gene(gene='recall_threshold',nodes=query_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.change_genes(gene_data={'recall_threshold': 0.3}, nodes=query_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_gene(gene='recall_threshold',nodes=query_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the Agent for Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hey Agent, recommend a car that has 4 cylinders that was manufactured in 1982\" - User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Okay, let me help you with that ... one moment please ...\" - Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.clear_wm(nodes=ingress_nodes)\n",
    "event = {\"vectors\": [], \"strings\": [\"cylinders|4\", \"model year|82\"], \"emotives\": {}}\n",
    "agent.observe(data=event,nodes=ingress_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "preds = agent.get_predictions(nodes=query_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mpg_mood_data = {'horsepower': 1.0, 'mpg': 1.0}\n",
    "hp_mpg_mood_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting prediction object based on the combine power of emotives, mood, and potential\n",
    "sorted_preds_utility = sorted(preds, key = lambda i: (((i['emotives']['mpg']*hp_mpg_mood_data['mpg'])+(i['emotives']['horsepower']*hp_mpg_mood_data['horsepower']))/2)*i['potential'], reverse=True)\n",
    "sorted_preds_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_preds_utility[0]['matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_preds_utility[0]['missing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the prediction ensemble is sorted by potential, prediction strength, but we're going to sort the prediction ensemble by utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_recommendation_specs = sorted_preds_utility[0]['missing']\n",
    "for spec in first_recommendation_specs:\n",
    "    if 'car name|' in spec:\n",
    "        print(f\"Based on your input the agent recommends '{spec.split('|')[1]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your emotional state your mood will change. You will favor one emotion over the other at certain times. For example, let's imagine that you've just read an article about imminent increase of gas prices. This new information affected your emotional state where you care more about mpg over horsepower.\n",
    "\n",
    "\"Hey Agent! Please update my horsepower mood weight to 0.5 and mpg mood weight to 1.5. Please give me a new car recommendation.\" - User\n",
    "\n",
    "\"Okay, let me help you with that ... one moment please ...\" - Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mpg_mood_data = {'horsepower': 0, 'mpg': 1.0}\n",
    "hp_mpg_mood_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting prediction object based on the combine power of emotives, mood, and potential\n",
    "sorted_preds_utility = sorted(preds, key = lambda i: (((i['emotives']['mpg']*hp_mpg_mood_data['mpg'])+(i['emotives']['horsepower']*hp_mpg_mood_data['horsepower']))/2)*i['potential'], reverse=True)\n",
    "sorted_preds_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_preds_utility[0]['matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_preds_utility[0]['missing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the prediction ensemble is sorted by potential, prediction strength, but we're going to sort the prediction ensemble by utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_recommendation_specs = sorted_preds_utility[0]['missing']\n",
    "for spec in second_recommendation_specs:\n",
    "    if 'car name|' in spec:\n",
    "        print(f\"Based on your input the agent recommends '{spec.split('|')[1]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You're all set to giving your Agent emotions. Feel free to change your mood from time to time in order to find predictions that fit your needs. Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Data sets: \n",
    "\n",
    "For classification:\n",
    "    - \"iris-flowers\"\n",
    "    \n",
    "For utility polarity:\n",
    "    - \"customer churn\"\n",
    "    \n",
    "For utility value:\n",
    "    - \"boston house prices\"\n",
    "    \n",
    "For emotives:\n",
    "    - \"auto mpg\"\n",
    "\n",
    "\n",
    "#### References:\n",
    "https://www.kaggle.com/arshid/iris-flower-dataset\n",
    "\n",
    "https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston\n",
    "\n",
    "https://www.kaggle.com/sagnikpatra/edadata\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/auto+mpg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
